{
  "name": "Music Analyzer",
  "tagline": "An Experiment in Machine Learning and Music Analysis",
  "body": "# Music Analyzer\r\nTyler Bowles, Daniel Griffin\r\n\r\nUtah State University\r\n\r\n## Abstract\r\nMusic is an expressive venue and as such, has significant emotive content. An algorithm to detect emotions could have commercial and therapeutic use.\r\nWe study the applicability of neural networks and genetic algorithms to predicting people’s emotional reactions to music by considering separetely the aural and lyrical components.\r\n\r\n## Problem Description\r\nIt is hypothesized that emotive content is encoded in audio and lyrical data.\r\nWe set out to implement an algorithm that recognizes emotional content in these components.\r\nWhile sentiment and tone analysis is usually done with classification in mind, this project is an experiment in predicting numerical measures of emotional states via regression.\r\n\r\nBoth individual words and songs from the data sets listed below were mapped to a two dimensional vector space called the arousal/valence scale.\r\nHigh arousal indicates high energy emotions such as anger or excitement.\r\nValence determines how positive an emotion is. For example, high valence would include emotions such as\r\ncalm and happy, while low valence may include sadness or fear.\r\n\r\nOur algorithms produce arousal/valence vectors for wav files and lyrical text files on a scale of -5 to 5 for both measures.\r\n\r\n## Data Acquisition\r\nThree primary data sources are used:\r\n\r\n1. A set of approximately 14,000 words that are mapped to arousal/valence vector space, with both coordinates ranging from 0 to 10. [4]\r\n2. Set of 1000 forty-five second clips of songs that are mapped to the arousal/valence vector space, with both coordinates ranging from 0 to 10. [3]\r\n3. 45 lyric files were manually found using the Google Search engine for various songs in the 1000 song data set.\r\n\r\nAfter data aquisition, the arousal/valence values in the annotations for the data set were shifted to a range of -5 to 5 in order to give more intuition behind the magnitude and sign of an emotion vector (e.g. the 0 vector is apathy, negative-valued valence indicates negative emotion). The 1000 song data set contained emotion annotations collected continuously throughout the duration of each audio clip, but only the mean arousal/valence annotations for each clip was used in our analysis.\r\n\r\n## Audio Analysis\r\n### Methods\r\nAudio is imported in the time domain and converted to the frequency domain via the Discrete Fourier Transform (DFT or FFT). The frequency distribution is smoothed using the discrete analog of an integral convolution:\r\n\r\n![Discrete Convolution Formula](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/DiscreteConvolution.PNG?raw=true)\r\n\r\nwhich helps eliminate noise and insignificant contributing frequencies.\r\nFour methods are then used for feature extraction for comparing predictive value:\r\n\r\n1. A third party library [1] extracts several common features generally used for signal processing, including energy spectra, Mel frequency cepstrum coefficients (MFCCs), and beats per minute estimation.\r\n2. Bins whose width grows logarithmically are centered around each of the frequencies corresponding to the 96 most common and easily heard western semitones [2]. The 96 bins are collapsed into the 12 western musical notes by summing over the average and maximum values within bins separated by octaves. Estimated BPM is appended.\r\n3. Maximum intensity and average intensity within each of the 96 semitone bins are calculated, and BPM is again used.\r\n4. The 20 highest intensity frequencies are identified and sorted.\r\nEach of these is then passed to a regression neural network which maps onto the 2-dimensional arousal/valence scale (each ranging from -5 to 5).\r\nThe network is trained based on a 1000 song database with available emotion-vector annotations [3].\r\n![Audio Analysis Diagram - Raw Audio to Fourier Transform to Convolution to Features](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/AudioAnalysisDiagram.PNG?raw=true)\r\n\r\n### Results\r\nMethod 4 (20 maxima) was severely problematic. Attempts to extract sufficiently distinct local maxima were unsuccessful, even after significant smoothing, and only noisy neighbors of the largest local maxima were extracted. This method was discarded. Feature extraction for the other 3 methods varied wildly in computational time. The NNs were trained on 2500 epochs, yielding the following mean errors:\r\n\r\n| Method        | Mean Error  |\r\n| ------------- |:------------|\r\n| 3rd Party     | 1.722       |\r\n| 12 Notes      | 1.611       |\r\n| 96 Semitones  | 2.733       |\r\n\r\nError was measured as distance from predicted vector to target vector, an example of some predicted vectors vs the target vectors is given in the figure below.\r\n\r\n![Example of Projected and Actual Emotion Vectors](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/ProjectedVectors.png?raw=true)\r\n\r\nAfter the initial drop in error in the first 50 epochs, additional training caused very gradual improvement with the exception of method 3, which seemed to overfit the data and worsen on new data over time.\r\nThe methods applied appear ill-equipped to improve beyond the error shown here, which may arise because of inadequacies in the algorithms or variance in the data. Among all models, method 2 performed best when tested with new data.\r\n\r\n![Neural Network Errors Throughout Training plot](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/ErrorConvergence.png?raw=true)\r\n\r\n## Lyric Analysis\r\n### Methods\r\n**Emotion Trajectory:**\r\n\r\nAn emotion vector for a lyric sheet is calculated by tracking a trajectory of the emotion in a song, and this information is utilized in several different way for comparison.\r\nA trajectory of the emotional state embedded in song lyrics is estimated in using variants of two primary\r\nmethods.\r\n\r\n1. Naive summation of the emotion vectors associated with each word\r\n2. A genetic algorithm accounting for context of words\r\n\r\nThe process of emotion trajectory calculation is depicted in the figure below.\r\n\r\n![Emotion trajectory in arousal/valence space](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/Trajectory.png?raw=true)\r\n\r\nIt is hypothesized that the genetic algorithm will be capable of accounting for grammatical information\r\nthat the naïve approach can not.\r\n\r\n**Genetic Algorithm:**\r\n* GA search space considers current word’s emotion vector, previous emotion state vectors,\r\ncurrent word’s part of speech, previous word’s part of speech, and word position in phrases.\r\n* Fitness is calculated based on average Euclidian distance from predicted AV vector to expected\r\nAV vector.\r\n* GA was trained using 45 lyric files mapped to AV vectors given in the 1000 song data set.\r\n* Two versions were used - genetic algorithm 1 utilizes all words in the lyric file whereas genetic algorithm 2 ignores all words without an associated emotion vector from the acquired dictionary.\r\n\r\n### Results\r\n1. Both genetic algorithms converge to an error better than a random guess.\r\n2. Randomly guessing a vector is more effective on this data than summing word emotion vectors in any of the naïve methods\r\n3. Resulting emotion vector predictions tend to be extreme, indicating that the lyrical component of this problem may be better characterized as a classification problem rather than one of regression.\r\n\r\n![Lyric Error Convergence](https://github.com/MightyTyGuy/MusicAnalyzer/blob/master/MDImages/LyricErrorConvergence.png?raw=true)\r\n\r\n| Method        | Mean Error  |\r\n| ------------- |:------------|\r\n| Naive + Errf  | 10.0577       |\r\n| Naive + Tanh  | 7.6251       |\r\n| Naive + Tanh + Decay  | 7.7224       |\r\n| Random Guess  | 5.201       |\r\n| Genetic Algorithm 1 | 3.3929       |\r\n| Genetic Algorithm 2  | 3.9895       |\r\n\r\n## Conclusions and Future Directions\r\n### Audio\r\nWe suggest that audio analysis method 3, which bins frequencies according to the 96 most common semitones found in western music, was susceptible to overfitting because the freedom of allowing 96 features causes the features to be very specific to each datum rather than representing a trend. While the error terms in all 3 methods do not converge to zero, they are small enough to suggest continued research into these methods. Possible approaches include:\r\n* Sampling and analyzing audio at smaller, random intervals to extract localized features\r\n* Examining whether this problem would be better suited for classification than regression\r\n* Training a NN for combined lyrical and audio prediction, as well as analysis of the relative predictive value of each\r\n* Investigation of the high variance in time required to extract features from audio files\r\n\r\n### Lyrics\r\nIt appears that a genetic algorithm will potentially be able to analyze emotion content more effectively\r\nthan a naïve summation approach. Our results indicate that the genetic algorithm is highly sensitive to the fitness and sigmoidal-type functions used to fit the data. Fine-tuning these parameters may result in improved accuracy.\r\nCompared with results found on the audio analysis, the lyrical analysis makes much weaker predictions, indicating that potentially emotional content is found primarily in audio rather than lyrical mediums.\r\nFurther work on updating the genetic algorithm's search space will potentially yield better results.\r\nTraining was poor due to a limited training data set, future work collecting an appropriate lyrical data\r\nset would enhance future results.\r\n\r\n\r\n## References\r\n[1] Giannakopoulos, Theodoros. \"PyAudioAnalysis: An Open-Source Python Library for Audio Signal Analysis.\" Plos One 10.12 (2015): n. pag. Web. Nov. 2016.\r\n\r\n[2] Suits, B. H. \"Frequencies of Musical Notes, A4 = 440 Hz.\" Physics of Music. Michigan Technological University, n.d. Web. 25 Oct. 2016. \r\n\r\n[3] Soleymani, M. et. al. (2014) 1000 Songs Database. \thttp://cvml.unige.ch/databases/emoMusic/dataset_manual.pdf\r\n\r\n[4] Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for\r\n13,915 English lemmas. Behavior research methods, 45(4), 1191-1207. Retrieved\r\nfrom http://crr.ugent.be/archives/1003\r\n\r\n[5] Misztal, J., & Indurkhya, B. (2014). Poetry generation system with an emotional personality.\r\nIn Proceedings of 5th International Conference on Computational Creativity, ICCC. Oudenne, A. M., & Chasins, S. E. Identifying the Emotional Polarity of Song Lyrics through Natural\r\nLanguage Processing",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}